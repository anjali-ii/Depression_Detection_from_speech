{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjali-ii/Depression_Detection_from_speech/blob/main/Speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgwdrBIsKq02",
        "outputId": "1ff25d58-d6b1-44b4-d12f-d252bbdf42e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kt5LIhXuLK6h"
      },
      "outputs": [],
      "source": [
        "# Define the directory where your audio files are stored\n",
        "base_dir = '/content/drive/My Drive/Audio'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv6XmEErOJch",
        "outputId": "9d1e4b86-895c-40e7-c881-b7594532aff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DYWPB-dOLLr",
        "outputId": "8c87d652-d204-4a4e-c37a-d571127e9ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyworld\n",
            "  Downloading pyworld-0.3.4.tar.gz (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.0/252.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyworld) (1.25.2)\n",
            "Requirement already satisfied: cython>=0.24 in /usr/local/lib/python3.10/dist-packages (from pyworld) (3.0.8)\n",
            "Building wheels for collected packages: pyworld\n",
            "  Building wheel for pyworld (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyworld: filename=pyworld-0.3.4-cp310-cp310-linux_x86_64.whl size=865274 sha256=66ef19d55fdbcbe1c41e82488c23b8a01a6e7996d840ea9c46b99c1b6581a1f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/09/8a/a1d79b73d59756f66e9bfe55a199840efc7473adb76ddacdfd\n",
            "Successfully built pyworld\n",
            "Installing collected packages: pyworld\n",
            "Successfully installed pyworld-0.3.4\n"
          ]
        }
      ],
      "source": [
        "pip install pyworld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wCBBLarDLVwn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import pyworld as pw\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_YasU9Lo71o7"
      },
      "outputs": [],
      "source": [
        "# Define parameters for feature extraction\n",
        "frame_length = 0.050  # Frame length in seconds\n",
        "hop_length = 0.030    # Hop length (frame shift) in seconds\n",
        "n_mels = 36           # Number of Mel bands\n",
        "fs = 16000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HvNYsZSH4qTU"
      },
      "outputs": [],
      "source": [
        "# Suppress UserWarning from librosa\n",
        "warnings.filterwarnings(\"ignore\", message=\"PySoundFile failed. Trying audioread instead.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwYjs0jw88nP",
        "outputId": "7eaf091c-4ce4-41e2-db98-f90ef175730a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.1)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.10.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.3.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VmVgD6U9RP-",
        "outputId": "b4c24145-f638-4cf7-aeac-7452f4717ee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: audioread in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "pip install audioread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HkgJlw8d9XM7"
      },
      "outputs": [],
      "source": [
        "# Suppress the warning\n",
        "warnings.filterwarnings('ignore', message='Deprecated as of librosa version 0.10.0.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rC5AxH5i-HWX"
      },
      "outputs": [],
      "source": [
        "# Suppress the FutureWarning\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9zvskE_mPaw5"
      },
      "outputs": [],
      "source": [
        "mfb_features = []\n",
        "participant_ids=[]\n",
        "f0_ap_features =[]\n",
        "# Iterate over each audio file\n",
        "for root, dirs, files in os.walk(base_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.m4a'):\n",
        "            # Construct the path to the audio file\n",
        "            audio_path = os.path.join(root, file)\n",
        "            #print(f\"Processing audio file: {audio_path}\")\n",
        "\n",
        "            # Load audio file\n",
        "            audio, sr = librosa.load(audio_path, mono=True)\n",
        "\n",
        "            # Resample to 16 kHz\n",
        "            target_sr = 16000\n",
        "            audio_resampled = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
        "\n",
        "            frame = audio_resampled.astype(np.double)\n",
        "            f0, timeaxis = pw.harvest(frame, fs, frame_period=30)\n",
        "            sp = pw.cheaptrick(frame, f0, timeaxis, fs)\n",
        "            ap = pw.d4c(frame, f0, timeaxis,fs)\n",
        "            ap_aggregated = np.mean(ap, axis=1)\n",
        "            # Calculate Mel filter bank features\n",
        "            mfb = pw.code_spectral_envelope(sp, fs, n_mels)\n",
        "\n",
        "            # Ensure all arrays have the same number of frames\n",
        "            min_frames = min(mfb.shape[0], f0.shape[0], ap_aggregated.shape[0])\n",
        "            mfb = mfb[:min_frames]\n",
        "            f0 = f0[:min_frames]\n",
        "            ap_aggregated = ap_aggregated[:min_frames]\n",
        "\n",
        "            # Standardization (Mean-variance normalization)\n",
        "            f0_mean = np.mean(f0)\n",
        "            f0_std = np.std(f0)\n",
        "            f0_normalized = (f0 - f0_mean) / f0_std\n",
        "\n",
        "            ap_aggregated_mean = np.mean(ap_aggregated)\n",
        "            ap_aggregated_std = np.std(ap_aggregated)\n",
        "            ap_aggregated_normalized = (ap_aggregated - ap_aggregated_mean) / ap_aggregated_std\n",
        "\n",
        "            mel_filter_bank_mean = np.mean(mfb, axis=0)\n",
        "            mel_filter_bank_std = np.std(mfb, axis=0)\n",
        "            mel_filter_bank_normalized = (mfb - mel_filter_bank_mean) / mel_filter_bank_std\n",
        "\n",
        "            # Append features to the list\n",
        "            mfb_features.append(np.concatenate([mel_filter_bank_normalized], axis=1))\n",
        "            f0_ap_features.append(np.concatenate([f0_normalized[:, np.newaxis], ap_aggregated_normalized[:, np.newaxis]], axis=1))\n",
        "            # Extract and append severity level\n",
        "            participant_id = os.path.basename(root)\n",
        "            participant_ids.extend([participant_id] * min_frames)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eOF5dqnN5pFe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "ae6b68ee-0ef2-4808-a0f4-64c8d07881a6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3b05f2b4ffc9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create DataFrame with F0 and Aperiodicity features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_f0_ap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf0_ap_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'F0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Aperiodicity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_f0_ap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'P_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparticipant_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Convert mfb_features into a single list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# Create DataFrame with F0 and Aperiodicity features\n",
        "df_f0_ap = pd.DataFrame(np.vstack(f0_ap_features), columns=['F0'] + ['Aperiodicity'])\n",
        "df_f0_ap['P_id'] = participant_ids\n",
        "\n",
        "# Convert mfb_features into a single list\n",
        "mfb_features_concatenated = [item for sublist in mfb_features for item in sublist]\n",
        "\n",
        "# Create DataFrame with concatenated MFB features and participant IDs\n",
        "df_mfb = pd.DataFrame({'MFB': mfb_features_concatenated, 'P_id': participant_ids})\n",
        "\n",
        "# Merge the two DataFrames based on participant IDs\n",
        "df = pd.merge(df_f0_ap, df_mfb, on='P_id')\n",
        "\n",
        "# Save merged DataFrame to a CSV file\n",
        "df.to_csv('frame_features_with_severity.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XliNOlIad4H9"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv('frame_features_with_severity.csv')\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df.tail())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZLQxme57vKs"
      },
      "outputs": [],
      "source": [
        "frame_features = []\n",
        "participant_ids=[]\n",
        "# Iterate over each audio file\n",
        "for root, dirs, files in os.walk(base_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.m4a'):\n",
        "            # Construct the path to the audio file\n",
        "            audio_path = os.path.join(root, file)\n",
        "            #print(f\"Processing audio file: {audio_path}\")\n",
        "\n",
        "            # Load audio file\n",
        "            audio, sr = librosa.load(audio_path, mono=True)\n",
        "\n",
        "            # Resample to 16 kHz\n",
        "            target_sr = 16000\n",
        "            audio_resampled = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
        "\n",
        "            frame = audio_resampled.astype(np.double)\n",
        "            f0, timeaxis = pw.harvest(frame, fs, frame_period=30)\n",
        "            sp = pw.cheaptrick(frame, f0, timeaxis, fs)\n",
        "            ap = pw.d4c(frame, f0, timeaxis,fs)\n",
        "            ap_aggregated = np.mean(ap, axis=1)\n",
        "            # Calculate Mel filter bank features\n",
        "            mfb = pw.code_spectral_envelope(sp, fs, n_mels)\n",
        "\n",
        "            # Ensure all arrays have the same number of frames\n",
        "            min_frames = min(mfb.shape[0], f0.shape[0], ap_aggregated.shape[0])\n",
        "            mfb = mfb[:min_frames]\n",
        "            f0 = f0[:min_frames]\n",
        "            ap_aggregated = ap_aggregated[:min_frames]\n",
        "\n",
        "            # Standardization (Mean-variance normalization)\n",
        "            f0_mean = np.mean(f0)\n",
        "            f0_std = np.std(f0)\n",
        "            f0_normalized = (f0 - f0_mean) / f0_std\n",
        "\n",
        "            ap_aggregated_mean = np.mean(ap_aggregated)\n",
        "            ap_aggregated_std = np.std(ap_aggregated)\n",
        "            ap_aggregated_normalized = (ap_aggregated - ap_aggregated_mean) / ap_aggregated_std\n",
        "\n",
        "            mel_filter_bank_mean = np.mean(mfb, axis=0)\n",
        "            mel_filter_bank_std = np.std(mfb, axis=0)\n",
        "            mel_filter_bank_normalized = (mfb - mel_filter_bank_mean) / mel_filter_bank_std\n",
        "\n",
        "            # Append features to the list\n",
        "            frame_features.append(np.concatenate([mel_filter_bank_normalized, f0_normalized[:, np.newaxis], ap_aggregated_normalized[:, np.newaxis]], axis=1))\n",
        "\n",
        "            # Extract and append severity level\n",
        "            participant_id = os.path.basename(root)\n",
        "            participant_ids.extend([participant_id] * min_frames)\n",
        "\n",
        "# Concatenate features and severity levels into a single DataFrame\n",
        "df1 = pd.DataFrame(np.vstack(frame_features), columns=[f'MFB_{i}' for i in range(n_mels)] + ['F0'] + ['Aperiodicity'])\n",
        "df1['P_id'] = participant_ids\n",
        "\n",
        "# Save DataFrame to a CSV file\n",
        "df1.to_csv('frame_features1_with_severity.csv', index=False)\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df1 = pd.read_csv('frame_features1_with_severity.csv')\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df1.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMvCTtFkXMcO",
        "outputId": "baa9797e-6366-49a4-c229-e4239ed428b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "75"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_rows = len(df)\n",
        "total_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm83OCNSoUlL"
      },
      "outputs": [],
      "source": [
        "# Load severity scores from Excel sheet\n",
        "file_path = '/content/drive/My Drive/PHQ.xlsx'\n",
        "severity_scores_df = pd.read_excel(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "OOOtt3nTrHYz",
        "outputId": "8dcaa2c8-a47e-4f50-9f97-2d435ecf7c01"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'P_id'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-5d9be30c0370>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Merge feature DataFrame with severity scores DataFrame based on participant ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmerged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseverity_scores_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'P_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m ) -> DataFrame:\n\u001b[0;32m--> 110\u001b[0;31m     op = _MergeOperation(\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1177\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m                         \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1848\u001b[0m             )\n\u001b[1;32m   1849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'P_id'"
          ]
        }
      ],
      "source": [
        "# Merge feature DataFrame with severity scores DataFrame based on participant ID\n",
        "merged_df = pd.merge(df, severity_scores_df, on='P_id', how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhM0py8usxDJ"
      },
      "outputs": [],
      "source": [
        "merged_df[56000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9hLfjh1jBw9"
      },
      "outputs": [],
      "source": [
        "# Extract features (X) and labels (Y)\n",
        "X = merged_df.drop(['P_id', 'Score', 'Category'], axis=1)  # Drop columns for ID, score, and category to get features\n",
        "Y = merged_df[['Score', 'Category']]  # Get columns for score and category as labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b75SvMW6wgOR"
      },
      "outputs": [],
      "source": [
        "print(\"Unique values in the 'Category' column:\", merged_df['Category'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J519IULAuQtK"
      },
      "outputs": [],
      "source": [
        "# Display the shapes of X and Y\n",
        "print(\"Shape of features (X):\", X.shape)\n",
        "print(\"Shape of labels (Y):\", Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOkfW3CoFa8X"
      },
      "outputs": [],
      "source": [
        "# Ensure that the feature vectors are reshaped appropriately for input into the CNN\n",
        "# The shape should be (num_samples, num_features, num_channels) for a single-channel CNN\n",
        "# If you have multiple channels (e.g., for different types of features), you'll need to adjust accordingly\n",
        "num_samples = X.shape[0]  # Number of samples\n",
        "num_features = X.shape[1]  # Number of features\n",
        "num_channels = 1  # Since it's a single-channel CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NQ_jbVwFc9c"
      },
      "outputs": [],
      "source": [
        "# Convert X to a NumPy array\n",
        "X = X.values\n",
        "\n",
        "# Reshape the feature vectors\n",
        "X = X.reshape(num_samples, num_features, num_channels)\n",
        "\n",
        "# Print the shapes of X and y to verify\n",
        "print(\"Shape of feature vectors (X):\", X.shape)\n",
        "print(\"Shape of labels (Y):\", Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9X7lrtmCHm6s"
      },
      "outputs": [],
      "source": [
        "# Extract feature columns\n",
        "features = merged_df.columns[:-1]  # Exclude the 'Severity' column\n",
        "\n",
        "# Plot bar graphs for each feature\n",
        "for feature in features:\n",
        "    if 'MFB_' in feature:  # Check if the feature is Mel Filter Bank\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        ax.set_title(f'Bar Graph of {feature}')\n",
        "        ax.set_xlabel('Category')\n",
        "        ax.set_ylabel('Mean Value')\n",
        "        merged_df.groupby('Category')[feature].mean().plot(kind='bar', ax=ax)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CkMh6b9Kegr"
      },
      "outputs": [],
      "source": [
        "# Extract feature columns\n",
        "features = merged_df.columns[:-1]  # Exclude the 'Severity' column\n",
        "\n",
        "# Plot bar graphs for each feature\n",
        "for feature in features:\n",
        "    if 'F0' in feature:  # Check if the feature is Mel Filter Bank\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        ax.set_title(f'Bar Graph of {feature}')\n",
        "        ax.set_xlabel('Severity')\n",
        "        ax.set_ylabel('Mean Value')\n",
        "        merged_df.groupby('Category')[feature].mean().plot(kind='bar', ax=ax)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BESodUtZLGoS"
      },
      "outputs": [],
      "source": [
        "# Extract feature columns\n",
        "features = merged_df.columns[:-1]  # Exclude the 'Severity' column\n",
        "\n",
        "# Plot bar graphs for each feature\n",
        "for feature in features:\n",
        "    if 'Aperiodicity' in feature:  # Check if the feature is Mel Filter Bank\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        ax.set_title(f'Bar Graph of {feature}')\n",
        "        ax.set_xlabel('Severity')\n",
        "        ax.set_ylabel('Mean Value')\n",
        "        merged_df.groupby('Category')[feature].mean().plot(kind='bar', ax=ax)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXIejLOg4tzD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8uzEx8BLOV1"
      },
      "outputs": [],
      "source": [
        "class MultiTaskCNN(nn.Module):\n",
        "    def __init__(self, num_mels=38):\n",
        "        super(MultiTaskCNN, self).__init__()\n",
        "        # First convolutional layer with three different kernels\n",
        "        self.conv1_1 = nn.Conv1d(in_channels=num_mels, out_channels=50, kernel_size=3, stride=3)\n",
        "        self.conv1_2 = nn.Conv1d(in_channels=num_mels, out_channels=50, kernel_size=10, stride=3)\n",
        "        self.conv1_3 = nn.Conv1d(in_channels=num_mels, out_channels=50, kernel_size=15, stride=3)\n",
        "\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv1d(in_channels=150, out_channels=50, kernel_size=3, stride=3)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(50 * 4, 150)  # Assuming input size of 4\n",
        "\n",
        "        # Output layers\n",
        "        self.fc_regression = nn.Linear(150, 1)  # Regression output\n",
        "        self.fc_classification = nn.Linear(150, 5)  # Classification output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First convolutional layer\n",
        "        x1 = nn.functional.relu(self.conv1_1(x))\n",
        "        x2 = nn.functional.relu(self.conv1_2(x))\n",
        "        x3 = nn.functional.relu(self.conv1_3(x))\n",
        "        x = torch.cat((x1, x2, x3), dim=1)  # Concatenate along channel dimension\n",
        "\n",
        "        # Second convolutional layer\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        x = nn.functional.max_pool1d(x, kernel_size=2)\n",
        "\n",
        "        # Print the shape of the tensor before flattening\n",
        "        print(\"Shape before flattening:\", x.shape)\n",
        "\n",
        "        # Flatten the output\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully connected layer\n",
        "        x = nn.functional.relu(self.fc(x))\n",
        "\n",
        "        # Output layers\n",
        "        regression_output = self.fc_regression(x)  # Regression output\n",
        "        classification_output = self.fc_classification(x)  # Classification output\n",
        "\n",
        "        return regression_output, classification_output\n",
        "\n",
        "# Create an instance of the MultiTaskCNN model\n",
        "model = MultiTaskCNN()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqLhLFMl7rCb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Y=Y.values\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoYeZw5eDd7e"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define loss functions for regression and classification\n",
        "criterion_regression = nn.MSELoss()\n",
        "criterion_classification = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOdZ8vzbh_Us"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQxeXkbn7wHq"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss_regression = 0.0\n",
        "    train_loss_classification = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        regression_output, classification_output = model(inputs)  # Forward pass\n",
        "        loss_regression = criterion_regression(regression_output, targets[:, 0])  # Calculate regression loss\n",
        "        loss_classification = criterion_classification(classification_output, targets[:, 1].long())  # Calculate classification loss\n",
        "        loss = loss_regression + loss_classification  # Total loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "        train_loss_regression += loss_regression.item() * inputs.size(0)\n",
        "        train_loss_classification += loss_classification.item() * inputs.size(0)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss_regression = 0.0\n",
        "    val_loss_classification = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            regression_output, classification_output = model(inputs)  # Forward pass\n",
        "            loss_regression = criterion_regression(regression_output, targets[:, 0])  # Calculate regression loss\n",
        "            loss_classification = criterion_classification(classification_output, targets[:, 1].long())  # Calculate classification loss\n",
        "            val_loss_regression += loss_regression.item() * inputs.size(0)\n",
        "            val_loss_classification += loss_classification.item() * inputs.size(0)\n",
        "\n",
        "    # Average losses\n",
        "    train_loss_regression /= len(train_loader.dataset)\n",
        "    train_loss_classification /= len(train_loader.dataset)\n",
        "    val_loss_regression /= len(val_loader.dataset)\n",
        "    val_loss_classification /= len(val_loader.dataset)\n",
        "\n",
        "    # Print epoch statistics\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss (Regression): {train_loss_regression:.4f}, '\n",
        "          f'Train Loss (Classification): {train_loss_classification:.4f}, '\n",
        "          f'Val Loss (Regression): {val_loss_regression:.4f}, '\n",
        "          f'Val Loss (Classification): {val_loss_classification:.4f}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'multi_task_cnn.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPaAl2iephAH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmx4cF8c9Vetr1xFnDJ5Gu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}